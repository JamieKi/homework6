---
title: "Homework 6"
author: "PSTAT 131/231"
output:
  pdf_document:
    toc: yes
  html_document:
    toc: yes
    toc_float: yes
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
options(tinytex.verbose = TRUE)
library(tidyverse)
library(tidymodels)
library(ISLR)
library(rpart.plot)
library(vip)
library(janitor)
library(randomForest)
library(xgboost)
library(ggplot2)
library(corrr)

```

## Tree-Based Models


**Note: Fitting ensemble tree-based models can take a little while to run. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit.**

### Exercise 1

Read in the data and set things up as in Homework 5:

- Use `clean_names()`
- Filter out the rarer Pok√©mon types
- Convert `type_1` and `legendary` to factors

Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.

Fold the training set using *v*-fold cross-validation, with `v = 5`. Stratify on the outcome variable.

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`:

- Dummy-code `legendary` and `generation`;
- Center and scale all predictors.

```{r}
pokemon <- read.csv(file = "~/Pokemon.csv")
head(pokemon)

library(janitor)
pokemon <- pokemon %>% clean_names()

pokemon_filter <- pokemon[pokemon$type_1 %in% c("Bug", "Fire", "Grass", "Normal", "Water", "Psychic"),]

pokemon_filter


names <- c('type_1' ,'legendary', 'generation')
pokemon_filter[,names] <- lapply(pokemon_filter[,names] , factor)
str(pokemon_filter)
print(str(pokemon_filter))

set.seed(3435)
pokemon_split <- initial_split(pokemon_filter, strata = "type_1")

pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)

pokemon_fold <- vfold_cv(pokemon_train, v = 5, strata = "type_1")
pokemon_fold

pokemon_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def , pokemon_train) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric())

```




### Exercise 2

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the continuous variables for this plot; justify your decision(s).*

What relationships, if any, do you notice? Do these relationships make sense to you?

We can see that there is no negative correlation between the variables. Additionally, each variable has the strongest correlation with the 'total' variable. These relationships do make sense for me.

```{r}

cor_pokemon_train <- pokemon_train %>%
  select(is.numeric) %>%
  cor(use = "pairwise.complete.obs", method = "pearson")
rplot(cor_pokemon_train)

```


### Exercise 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in Lab 7 -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`.

Print an `autoplot()` of the results. What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?

A single decision tree perform better with a smaller complexity penality because when the values are smaller, the 'roc_auc' values are larger. Larger 'roc_auc' values brings a better decision tree.


```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("classification")

tree_workflow <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(pokemon_recipe)

set.seed(3435)
pokemon_fold <- vfold_cv(pokemon_train)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  tree_workflow,
  resamples = pokemon_fold,
  grid = param_grid,
  metrics = metric_set(roc_auc)
)

autoplot(tune_res)





```


### Exercise 4

What is the `roc_auc` of your best-performing pruned decision tree on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

The 'roc_auc' of my best-performing pruned decision tree on the folds would be 0.6683159.

```{r}
collect_metrics(tune_res)
arrange(tune_res)
best_complexity <- select_best(tune_res)

best_complexity

```



### Exercise 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.


```{r}
class_tree_final <- finalize_workflow(tree_workflow, best_complexity)

class_tree_final_fit <- fit(class_tree_final, data = pokemon_train)

class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```


### Exercise 5

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

The hyperparamter 'mtry' represent the number of variables randomly sampled as candidates at each split. 'trees' represent the number of trees to grow and 'min_n' represent the number of observations needed to keep splitting nodes.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**

'mtry = 8' would represent the creation of the tree and looking at 8 of my features before going into the next step.


```{r}
class_forest_spec <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

param_grid2 <- grid_regular(mtry(range = c(1, 8)), trees(range = c(1,8)), min_n(range = c(1,8)),  levels = 8)

forest_workflow <- workflow() %>%
  add_model(class_forest_spec %>% set_args(mtry = tune(), trees = tune(), min_n = tune())) %>%
  add_recipe(pokemon_recipe)

####

#class_forest_spec <- rand_forest() %>%
#  set_engine("ranger", importance = "impurity") %>%
#  set_mode("classification")

#param_grid2_trees <- grid_regular(trees(range = c(1, 8)),  levels = 8)

#forest_workflow_trees <- workflow() %>%
#  add_model(class_forest_spec %>% set_args(trees = tune())) %>%
#  add_recipe(pokemon_recipe)

####
#class_forest_spec <- rand_forest() %>%
#  set_engine("ranger", importance = "impurity") %>%
#  set_mode("classification")

#param_grid2_min_n <- grid_regular(min_n(range = c(1, 8)),  levels = 8)

#forest_workflow_min_n <- workflow() %>%
#  add_model(class_forest_spec %>% set_args(min_n = tune())) %>%
#  add_recipe(pokemon_recipe)

```


### Exercise 6

Specify `roc_auc` as a metric. Tune the model and print an `autoplot()` of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?

```{r}

tune_res_forest <- tune_grid(
  forest_workflow,
  resamples = pokemon_fold,
  grid = param_grid2,
  metrics = metric_set(roc_auc)
)

autoplot(tune_res_forest)

# ###
#
# tune_res_forest <- tune_grid(
#   forest_workflow_trees,
#   resamples = pokemon_fold,
#   grid = param_grid2_trees,
#   metrics = metric_set(roc_auc)
# )
#
# autoplot(tune_res_forest)
#
# ####
#
# tune_res_forest <- tune_grid(
#   forest_workflow_min_n,
#   resamples = pokemon_fold,
#   grid = param_grid2_min_n,
#   metrics = metric_set(roc_auc)
# )
#
# autoplot(tune_res_forest)



```


### Exercise 7

What is the `roc_auc` of your best-performing random forest model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

The 'roc_auc' of the best-performing random forest model on the folds would be 0.7219057.

```{r}
collect_metrics(tune_res_forest)
arrange(tune_res_forest)
best_complexity2 <- select_best(tune_res_forest)
best_complexity2
```

### Exercise 8

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.

Which variables were most useful? Which were least useful? Are these results what you expected, or not?



```{r}

class_tree_final_fit %>%
  pull_workflow_fit() %>%
  vip()
```

### Exercise 9

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results.
What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

The 'roc_auc' of my best-performing boosted tree model on the folds is 0.6944903.

```{r}

boost_spec <- boost_tree(trees = c(10,2000), tree_depth = 4) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

param_grid_boost <- grid_regular(trees(range = c(10, 2000)),  levels = 10)

boost_workflow <- workflow() %>%
  add_model(boost_spec %>% set_args(trees = tune())) %>%
  add_recipe(pokemon_recipe)

tune_res_boost <- tune_grid(
  boost_workflow,
  resamples = pokemon_fold,
  grid = param_grid_boost,
  metrics = metric_set(roc_auc)
)

autoplot(tune_res_boost)

collect_metrics(tune_res_boost)
arrange(tune_res_boost)
best_complexity3 <- select_best(tune_res_boost)
best_complexity3
```




### Exercise 10

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set.

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

Which classes was your model most accurate at predicting? Which was it worst at?
Note that classifiers that give corners closer to the top left means a good prediction. Therefore, classes "Bug", "Fire", "Grass", "Normal" were the most accurate at predicting and "Psychic" and "Water" were the worst at predicting.


```{r}

set.seed(1)
df <- data.frame(best_performing = c(0.6683159, 0.7219057, 0.6944903),
                 models <- c("pruned tree model", "random forest model", "boosted tree model"))

head(df)

#fit it to the testing set
best_complexity <- select_best(tune_res)

class_tree_final <- finalize_workflow(forest_workflow, best_complexity2)

class_tree_final_fit <- fit(class_tree_final, data = pokemon_test)

#AUC value
pred_result <- augment(class_tree_final_fit, new_data = pokemon_test)
auc <- roc_auc(data = pred_result, truth = type_1, estimate = c(.pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Water, .pred_Psychic ), estimator = "macro_weighted")
auc

#roc curve
augment(class_tree_final_fit, new_data = pokemon_test) %>%
  roc_curve(type_1, estimate = .pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Water, .pred_Psychic) %>%
  autoplot()


#confusion matrix heat map
augment(class_tree_final_fit, new_data = pokemon_test) %>%
  conf_mat(truth = type_1, estimate = .pred_class) %>%
  autoplot(type = "heatmap")



# best_performing <- c(best_complexity, best_complexity2, best_complexity3)
# models <- c("pruned tree model", "random forest model", "boosted tree model")
# results <- tibble(best_model = best_performing, models = models)



#AUC value
pred_result <- augment(class_tree_final_fit, new_data = pokemon_test)
auc <- roc_auc(data = pred_result, truth = type_1, estimate = c(.pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Water, .pred_Psychic ), estimator = "macro_weighted")
auc

```